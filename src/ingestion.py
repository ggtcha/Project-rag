import os
import pandas as pd
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing import List, Dict
import re

load_dotenv()

# ============================================================================
# Configuration
# ============================================================================
DB_URL = f"postgresql+psycopg2://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}"
COLLECTION = os.getenv("COLLECTION_NAME")
EMBED_MODEL = "mxbai-embed-large"
OLLAMA_BASE_URL = "http://localhost:11434"

# ============================================================================
# Utility Functions
# ============================================================================
def clean_text(value):
    """‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏∞‡πÇ‡∏î‡∏¢‡∏¢‡∏±‡∏á‡∏£‡∏±‡∏Å‡∏©‡∏≤ Serial Number ‡πÑ‡∏ß‡πâ"""
    if pd.isna(value) or value is None:
        return None
    
    val = str(value).strip()
    useless = ['nan', 'none', 'null', 'n/a', '', '‡πÑ‡∏°‡πà‡∏°‡∏µ', '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•']
    junk_patterns = ['dtype: object', 'dtype: int64', 'Name:', 'Unnamed:']
    
    if val.lower() in useless:
        return None
    
    for pattern in junk_patterns:
        val = val.replace(pattern, '')
        
    return val.strip() or None

def extract_searchable_codes(row_dict: Dict) -> List[str]:
    """‡∏î‡∏∂‡∏á‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"""
    codes = []
    priority_fields = ['serial', 's/n', 'sn', 'model', 'part', 'asset', 'code']
    
    for col, val in row_dict.items():
        col_lower = str(col).lower()
        
        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô field ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if any(pf in col_lower for pf in priority_fields):
            clean_val = clean_text(val)
            if clean_val:
                # ‡∏î‡∏∂‡∏á‡∏£‡∏´‡∏±‡∏™‡∏î‡πâ‡∏ß‡∏¢ regex
                found_codes = re.findall(r'[A-Z0-9]+-[A-Z0-9-/]+|[A-Z]*\d{5,}', str(clean_val).upper())
                codes.extend(found_codes)
    
    return list(set(codes))  # ‡∏•‡∏ö duplicate

def create_content_body(row_dict: Dict, label: str) -> str:
    """‡∏à‡∏±‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"""
    groups = {
        "Identification": [], 
        "Location": [],      
        "Responsibility": [], 
        "Status": [],        
        "Technical": [],
        "Others": []
    }
    
    keywords = {
        "Identification": ['serial', 's/n', 'sn', 'part', 'model', '‡∏£‡∏´‡∏±‡∏™', '‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç', 'code', 'name', 'asset', 'item'],
        "Location": ['location', '‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà', '‡πÇ‡∏ã‡∏ô', 'shelf', 'zone', 'room', 'building', '‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£'],
        "Responsibility": ['responsible', 'owner', '‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö', '‡πÄ‡∏à‡πâ‡∏≤‡∏Ç‡∏≠‡∏á', 'person', 'user', 'department'],
        "Status": ['status', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞', 'condition', 'state', 'available'],
        "Technical": ['spec', 'description', '‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î', '‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥', 'brand', 'manufacturer']
    }

    for col, raw_val in row_dict.items():
        val = clean_text(raw_val)
        if not val: 
            continue
        
        line = f"{col}: {val}"
        found = False
        
        for group, keys in keywords.items():
            if any(k in str(col).lower() for k in keys):
                groups[group].append(line)
                found = True
                break
        
        if not found: 
            groups["Others"].append(line)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    sections = [f"=== Category: {label} ==="]
    
    titles = {
        "Identification": "## ‡∏£‡∏´‡∏±‡∏™‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤", 
        "Location": "## ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà", 
        "Responsibility": "## ‡∏ú‡∏π‡πâ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö", 
        "Status": "## ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞", 
        "Technical": "## ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ",
        "Others": "## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
    }
    
    for key, title in titles.items():
        if groups[key]:
            sections.extend([f"\n{title}", *groups[key]])
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÑ‡∏ß‡πâ‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠
    searchable_codes = extract_searchable_codes(row_dict)
    if searchable_codes:
        sections.append(f"\n## Searchable Codes: {', '.join(searchable_codes)}")
    
    return "\n".join(sections)

def process_sheet(file: str, sheet: str, label: str) -> List[Document]:
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Sheet ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Documents"""
    try:
        df = pd.read_excel(file, sheet_name=sheet)
        
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        df = df.dropna(how='all').drop_duplicates()
        df.columns = [str(c).strip() for c in df.columns]
        
        print(f"üìã Processing sheet: {sheet}")
        print(f"   - Total rows: {len(df)}")
        print(f"   - Columns: {list(df.columns)}")
        
        docs = []
        skipped = 0
        
        for idx, row in df.iterrows():
            data = row.to_dict()
            content = create_content_body(data, label)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if len(content) < 30: 
                skipped += 1
                continue
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î
            meta = {}
            for k, v in data.items():
                clean_v = clean_text(v)
                if clean_v:
                    meta[k.lower().replace(' ', '_')] = clean_v
            
            meta.update({
                "sheet": sheet, 
                "category": label, 
                "row_index": int(idx)
            })
            
            docs.append(Document(page_content=content, metadata=meta))
        
        print(f"   ‚úÖ Created: {len(docs)} documents")
        print(f"   ‚ö†Ô∏è Skipped: {skipped} rows (insufficient data)")
        
        return docs
        
    except Exception as e:
        print(f"‚ùå Error processing {sheet}: {e}")
        import traceback
        traceback.print_exc()
        return []

def verify_documents(docs: List[Document]):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤"""
    print("\nüîç Verifying Documents Quality...")
    
    if not docs:
        print("‚ùå No documents to verify!")
        return False
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    sample = docs[0]
    print(f"\nüìÑ Sample Document:")
    print(f"Content preview: {sample.page_content[:200]}...")
    print(f"Metadata: {sample.metadata}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    codes_found = 0
    for doc in docs[:10]:  # ‡∏ï‡∏£‡∏ß‡∏à 10 ‡∏≠‡∏±‡∏ô‡πÅ‡∏£‡∏Å
        if "Searchable Codes:" in doc.page_content:
            codes_found += 1
    
    print(f"\nüìä Statistics:")
    print(f"   - Total documents: {len(docs)}")
    print(f"   - Documents with codes: {codes_found}/10 (sample)")
    print(f"   - Average length: {sum(len(d.page_content) for d in docs) / len(docs):.0f} chars")
    
    return True

# ============================================================================
# Main Ingestion
# ============================================================================
def run_ingestion():
    print("="*60)
    print("üöÄ Starting Data Ingestion Process")
    print("="*60)
    
    excel_file = "data/data_inventory.xlsx" 
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
    if not os.path.exists(excel_file):
        print(f"‚ùå File not found: {excel_file}")
        return
    
    target_sheets = [
        ("Spare", "‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á"), 
        ("Obsolete", "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô/‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡∏™‡∏†‡∏≤‡∏û")
    ]
    
    print(f"\nüìÇ Reading from: {excel_file}")
    print(f"üìä Target sheets: {[s[0] for s in target_sheets]}\n")
    
    # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    all_docs = []
    for sheet, label in target_sheets:
        docs = process_sheet(excel_file, sheet, label)
        all_docs.extend(docs)

    if not all_docs:
        print("\n‚ùå No valid documents found!")
        return

    print(f"\n{'='*60}")
    print(f"üì¶ Total Documents: {len(all_docs)}")
    print(f"{'='*60}\n")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    if not verify_documents(all_docs):
        print("‚ùå Document verification failed!")
        return
    
    # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    print("\n‚úÇÔ∏è Splitting documents into chunks...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î chunk
        chunk_overlap=150,     # ‡πÄ‡∏û‡∏¥‡πà‡∏° overlap
        separators=[
            "=== Category:",
            "\n## ",
            "\n\n",
            "\n",
            " "
        ],
        length_function=len,
    )
    
    chunks = splitter.split_documents(all_docs)
    print(f"‚úÖ Created {len(chunks)} chunks")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á chunk
    if chunks:
        print(f"\nüìÑ Sample Chunk:")
        print(f"{chunks[0].page_content[:300]}...")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    try:
        print(f"\n{'='*60}")
        print("üóÑÔ∏è Storing in Vector Database...")
        print(f"{'='*60}\n")
        
        embeds = OllamaEmbeddings(
            model=EMBED_MODEL, 
            base_url=OLLAMA_BASE_URL
        )
        
        print("‚è≥ This may take a few minutes...")
        
        PGVector.from_documents(
            embedding=embeds,
            documents=chunks,
            collection_name=COLLECTION,
            connection_string=DB_URL,
            pre_delete_collection=True,
            use_jsonb=True
        )
        
        print(f"\n{'='*60}")
        print("‚úÖ INGESTION COMPLETED SUCCESSFULLY!")
        print(f"{'='*60}")
        print(f"üìä Collection: {COLLECTION}")
        print(f"üì¶ Total chunks: {len(chunks)}")
        print(f"üéØ Ready for queries!")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\n{'='*60}")
        print(f"‚ùå DATABASE ERROR")
        print(f"{'='*60}")
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_ingestion()