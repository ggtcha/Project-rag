import os
import gc
import pandas as pd
from typing import List, Dict, Optional
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# ============================================================================
# Configuration
# ============================================================================
DB_URL = f"postgresql+psycopg2://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}"
COLLECTION = os.getenv("COLLECTION_NAME")
EMBED_MODEL = "nomic-embed-text"
OLLAMA_BASE_URL = "http://localhost:11434"

# ‡πÑ‡∏ü‡∏•‡πå Excel ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
INVENTORY_FILE = "data/data_inventory.xlsx"

# Sheets ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)
TARGET_SHEETS = [
    ("Spare", "‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà/‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏≥‡∏£‡∏≠‡∏á"),
    ("Obsolete", "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"),
]

# ============================================================================
# TEXT CLEANING
# ============================================================================
USELESS_VALUES = {
    "nan", "none", "null", "n/a", "", "-", "‡πÑ‡∏°‡πà‡∏°‡∏µ", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "N/A"
}

def clean_text(value: Optional[str]) -> Optional[str]:
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
    if value is None or pd.isna(value):
        return None
    
    text = str(value).strip()
    if text.lower() in USELESS_VALUES or text == "-":
        return None
    
    return text

# ============================================================================
# DEVICE CATEGORY DETECTION
# ============================================================================
def detect_device_category(model: str) -> str:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å Model"""
    if not model:
        return "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ"
    
    model_lower = model.lower()
    
    if "thinkpad" in model_lower or "laptop" in model_lower or "elitebook" in model_lower:
        return "Laptop/Notebook"
    elif "thinkcentre" in model_lower or "optiplex" in model_lower or "prodesk" in model_lower:
        return "Desktop Computer"
    elif "thinkstation" in model_lower or "workstation" in model_lower:
        return "Workstation"
    elif "switch" in model_lower:
        return "Network Switch"
    elif "router" in model_lower:
        return "Router"
    elif "access point" in model_lower or "wifi" in model_lower or "wireless" in model_lower:
        return "Access Point/WiFi"
    elif "neverstop" in model_lower or "printer" in model_lower:
        return "Printer"
    elif "mac mini" in model_lower or "mac" in model_lower:
        return "Mac Computer"
    elif "lenovo" in model_lower and ("v510z" in model_lower or "aio" in model_lower):
        return "All-in-One PC"
    elif "air" in model_lower and "4g" in model_lower:
        return "4G Router/Modem"
    else:
        return "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå IT ‡∏≠‡∏∑‡πà‡∏ô‡πÜ"

# ============================================================================
# IMPROVED CONTENT BUILDER
# ============================================================================
def build_inventory_content(row: Dict, sheet_label: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á content ‡∏ó‡∏µ‡πà rich ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤"""
    
    parts = []
    
    # Header
    parts.append(f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: IT Asset Inventory")
    parts.append(f"‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: {sheet_label}")
    parts.append("")
    
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å
    model = clean_text(row.get('Model'))
    model_no = clean_text(row.get('Model No.'))
    model_name = clean_text(row.get('Model Name'))
    serial = clean_text(row.get('Serial'))
    status = clean_text(row.get('Status'))
    lifetime = clean_text(row.get('Lifetime'))
    purchased = clean_text(row.get('Purchased'))
    order_num = clean_text(row.get('Order Number'))
    asset_no = clean_text(row.get('Asset No'))
    locations = clean_text(row.get('Locations'))
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå
    device_category = detect_device_category(model) if model else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
    parts.append(f"‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå: {device_category}")
    parts.append("")
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤/‡∏£‡∏∏‡πà‡∏ô (‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤)
    parts.append("## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∏‡πà‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤")
    
    if model:
        parts.append(f"‡∏£‡∏∏‡πà‡∏ô: {model}")
        parts.append(f"Model: {model}")
        parts.append(f"‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤: {model}")
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
        model_compact = model.replace(" ", "")
        parts.append(f"‡∏£‡∏´‡∏±‡∏™: {model_compact}")
    
    if model_no:
        parts.append(f"Model Number: {model_no}")
        parts.append(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏£‡∏∏‡πà‡∏ô: {model_no}")
        parts.append(f"‡∏£‡∏´‡∏±‡∏™‡∏£‡∏∏‡πà‡∏ô: {model_no}")
    
    if model_name:
        parts.append(f"Model Name: {model_name}")
        parts.append(f"‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏∏‡πà‡∏ô: {model_name}")
    
    parts.append("")
    parts.append("## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô")
    
    if serial:
        parts.append(f"Serial Number: {serial}")
        parts.append(f"S/N: {serial}")
        parts.append(f"Serial: {serial}")
        parts.append(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ã‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡∏•: {serial}")
        parts.append(f"‡∏ã‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡∏•: {serial}")
    
    if asset_no:
        parts.append(f"Asset Number: {asset_no}")
        parts.append(f"Asset No: {asset_no}")
        parts.append(f"‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô: {asset_no}")
        parts.append(f"‡∏£‡∏´‡∏±‡∏™‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô: {asset_no}")
    
    parts.append("")
    parts.append("## ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á")
    
    if status:
        parts.append(f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {status}")
        parts.append(f"Status: {status}")
        
        # ‡πÅ‡∏õ‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
        if "spare" in status.lower():
            parts.append("‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏´‡∏•‡πà‡∏™‡∏≥‡∏£‡∏≠‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô available spare")
        elif "obsolete" in status.lower():
            if "deployable" in status.lower():
                parts.append("‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ obsolete but still working")
            elif "deployed" in status.lower():
                parts.append("‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà obsolete and in use")
    
    if locations:
        parts.append(f"‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á: {locations}")
        parts.append(f"Location: {locations}")
        parts.append(f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà: {locations}")
        parts.append(f"‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {locations}")
    
    parts.append("")
    parts.append("## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    if lifetime:
        parts.append(f"‡∏≠‡∏≤‡∏¢‡∏∏‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: {lifetime}")
        parts.append(f"Lifetime: {lifetime}")
        parts.append(f"‡∏≠‡∏≤‡∏¢‡∏∏: {lifetime}")
    
    if purchased:
        parts.append(f"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠: {purchased}")
        parts.append(f"Purchased: {purchased}")
        parts.append(f"‡∏ã‡∏∑‡πâ‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠: {purchased}")
    
    if order_num:
        parts.append(f"‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÉ‡∏ö‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠: {order_num}")
        parts.append(f"Order Number: {order_num}")
        parts.append(f"PO: {order_num}")
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÜ
    parts.append("")
    parts.append("## ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
    
    important_cols = ['Model', 'Model No.', 'Model Name', 'Serial', 'Status', 
                      'Lifetime', 'Purchased', 'Order Number', 'Asset No', 'Locations']
    
    for col, val in row.items():
        if col not in important_cols:
            clean_val = clean_text(val)
            if clean_val:
                parts.append(f"{col}: {clean_val}")
    
    # Context Hint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
    parts.append("")
    parts.append("## ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
    search_terms = []
    
    if model:
        search_terms.append(model)
        search_terms.append(model.replace(" ", ""))
    if serial:
        search_terms.append(f"Serial {serial}")
    if asset_no:
        search_terms.append(f"Asset {asset_no}")
    if locations:
        search_terms.append(f"‡∏ó‡∏µ‡πà {locations}")
    if device_category:
        search_terms.append(device_category)
    
    parts.append(" | ".join(search_terms))
    
    return "\n".join(parts)

# ============================================================================
# DOCUMENT LOADER
# ============================================================================
def load_inventory_documents(
    file_path: str,
    sheet_configs: List[tuple]
) -> List[Document]:
    """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• inventory ‡∏à‡∏≤‡∏Å Excel"""
    
    all_docs = []
    
    for sheet_name, label in sheet_configs:
        print(f"\n  üìÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î sheet: {sheet_name}")
        
        try:
            df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                dtype=str,
            ).dropna(how="all")
            
            # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ã‡πâ‡∏≥
            df = df.drop_duplicates()
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
            df.columns = [str(c).strip() for c in df.columns]
            
            print(f"     ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(df)} ‡πÅ‡∏ñ‡∏ß")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Documents
            doc_count = 0
            for idx, row in df.iterrows():
                data = row.to_dict()
                
                # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Model ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢
                if not clean_text(data.get('Model')):
                    continue
                
                content = build_inventory_content(data, label)
                
                # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
                if len(content) < 50:
                    continue
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata
                metadata = {
                    "source": "inventory",
                    "sheet": sheet_name,
                    "category": label,
                    "row": int(idx),
                    "model": clean_text(data.get('Model')),
                    "serial": clean_text(data.get('Serial')),
                    "asset_no": clean_text(data.get('Asset No')),
                    "location": clean_text(data.get('Locations')),
                    "status": clean_text(data.get('Status')),
                    "device_type": detect_device_category(clean_text(data.get('Model', ''))),
                }
                
                # ‡∏•‡∏ö‡∏Ñ‡πà‡∏≤ None
                metadata = {k: v for k, v in metadata.items() if v is not None}
                
                all_docs.append(
                    Document(
                        page_content=content,
                        metadata=metadata,
                    )
                )
                doc_count += 1
            
            print(f"     ‚úì ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ {doc_count} documents")
            
            del df
            gc.collect()
            
        except Exception as e:
            print(f"     ‚ùå Error loading {sheet_name}: {e}")
            continue
    
    return all_docs

# ============================================================================
# MAIN INGESTION
# ============================================================================
def ingest_real_inventory():
    
    print("="*70)
    print(" IT SUPPORT KNOWLEDGE BASE - INGESTION PROCESS")
    print("="*70)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå
    if not os.path.exists(INVENTORY_FILE):
        print(f"\n‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {INVENTORY_FILE}")
        print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå data/")
        return
    
    print(f"\n ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {INVENTORY_FILE}")
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    all_docs = load_inventory_documents(INVENTORY_FILE, TARGET_SHEETS)
    
    if not all_docs:
        print("\n ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà valid")
        return
    
    print(f"\n ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_docs)} documents")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    sheet_stats = {}
    for doc in all_docs:
        sheet = doc.metadata.get('sheet', 'Unknown')
        sheet_stats[sheet] = sheet_stats.get(sheet, 0) + 1
    
    print("\n ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏° Sheet:")
    for sheet, count in sheet_stats.items():
        print(f"   - {sheet}: {count} documents")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
    print("\n ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Document ‡πÅ‡∏£‡∏Å:")
    print("-"*70)
    print(all_docs[0].page_content[:600])
    print("...")
    print("-"*70)
    
    # Split documents
    print("\n ‡∏Å‡∏≥‡∏•‡∏±‡∏á split documents...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏£‡∏ö
        chunk_overlap=250,
        separators=["\n\n## ", "\n\n", "\n", ". ", " "],
    )
    
    chunks = splitter.split_documents(all_docs)
    print(f" ‡∏™‡∏£‡πâ‡∏≤‡∏á {len(chunks)} chunks")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á chunk
    print("\n ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á chunk ‡πÅ‡∏£‡∏Å:")
    print("-"*70)
    print(chunks[0].page_content[:500])
    print("...")
    print("-"*70)
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏•‡∏ö
    total_docs = len(all_docs)
    total_chunks = len(chunks)
    
    del all_docs
    gc.collect()
    
    # Embedding + Store
    print(f"\n ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á PGVector...")
    print(f"   Collection: {COLLECTION}")
    print(f"   Embedding Model: {EMBED_MODEL}")
    
    embeddings = OllamaEmbeddings(
        model=EMBED_MODEL,
        base_url=OLLAMA_BASE_URL,
    )
    
    try:
        PGVector.from_documents(
            documents=chunks,
            embedding=embeddings,
            collection_name=COLLECTION,
            connection_string=DB_URL,
            pre_delete_collection=True,  # ‡∏•‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤
        )
        
        print("\n" + "="*70)
        print(" INGESTION ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        print("="*70)
        print(f" Collection: {COLLECTION}")
        print(f" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Documents: {total_docs}")
        print(f" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Chunks: {total_chunks}")
        print(f" Chunk Size: 1500 characters")
        print(f" Overlap: 250 characters")
        print("="*70)
    except Exception as e:
        print(f"\n Error during ingestion: {e}")
        
# ============================================================================
# ENTRY POINT
# ============================================================================
if __name__ == "__main__":
    ingest_real_inventory()